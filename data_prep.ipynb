{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "set_dir = 'test'\n",
    "f = open('{}.json'.format(set_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_json = json.load(f)\n",
    "f.close()\n",
    "len(dat_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/home/haozhez2/anaconda3/envs/asr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "# data, sr = sf.read('1.wav')\n",
    "# data = librosa.resample(data[:,0],sr,16000)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model_audio = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "\n",
    "# i= feature_extractor(data, return_tensors=\"pt\", sampling_rate=16000)\n",
    "# #previous are in dataloader\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   o= model_audio(i.input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_441495/3862239507.py:16: FutureWarning: Pass orig_sr=48000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  tmp_audio = librosa.resample(tmp_audio[:,0],sr,16000)\n",
      "/tmp/ipykernel_441495/3862239507.py:16: FutureWarning: Pass orig_sr=48000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  tmp_audio = librosa.resample(tmp_audio[:,0],sr,16000)\n",
      "100%|██████████| 1000/1000 [07:30<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = set_dir+'/'\n",
    "idx = 0\n",
    "dat_combined_json = []\n",
    "for dat in tqdm(dat_json):\n",
    "    text = dat['description']\n",
    "    audio_list = dat['audio_ids']\n",
    "    tar_id = dat['target_id']\n",
    "    final_feat = []\n",
    "    final_target = []\n",
    "    final_hidden_state = []\n",
    "    for audio_name in audio_list:\n",
    "        tmp_audio, sr = sf.read(file_path+audio_name+'.wav')\n",
    "        tmp_audio = librosa.resample(tmp_audio[:,0],sr,16000)\n",
    "        i= feature_extractor(tmp_audio, return_tensors=\"pt\", sampling_rate=16000)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o= model_audio(i.input_values.to(device))\n",
    "\n",
    "        cur_feat = o.extract_features.cpu().numpy()\n",
    "        cur_hidden_state = o.last_hidden_state.cpu().numpy()\n",
    "        B, T, D = cur_feat.shape\n",
    "        # print(B,T,D)\n",
    "\n",
    "\n",
    "        tmp_tar = np.zeros(T)\n",
    "        if audio_name == tar_id:\n",
    "            tmp_tar += 1\n",
    "\n",
    "        final_feat.append(np.squeeze(cur_feat))\n",
    "        final_hidden_state.append(np.squeeze(cur_hidden_state))\n",
    "        final_target.append(tmp_tar)\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['id'] = idx\n",
    "    tmp_dict['audio_ids'] = audio_list\n",
    "    tmp_dict['target_id'] = tar_id\n",
    "    tmp_dict['description'] = text\n",
    "    final_feat = np.vstack(final_feat)\n",
    "    final_hidden_state = np.vstack(final_hidden_state)\n",
    "    final_target = np.hstack(final_target)\n",
    "    # print(final_feat.shape, final_hidden_state.shape, final_target.shape)\n",
    "    np.save('./'+set_dir+'/'+str(idx)+'_feat', final_feat)\n",
    "    np.save('./'+set_dir+'/'+str(idx)+'_hidden_state', final_hidden_state)\n",
    "    np.save('./'+set_dir+'/'+str(idx)+'_target', final_target)\n",
    "    dat_combined_json.append(tmp_dict)\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('{}_combined.json'.format(set_dir), \"w\") as outfile:\n",
    "    json.dump(dat_combined_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dat_combined_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4c4b94d2c2ebce001fd4ad053dd500eb853c4ed6e1be56e669d125c68b3ff64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
