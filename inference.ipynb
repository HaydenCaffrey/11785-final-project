{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "./test/0_feat.npy\n",
      "./test/1_feat.npy\n",
      "./test/2_feat.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummaryX import summary\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_text = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "class RASDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, file_pth, partition = 'train', subset= None): \n",
    "        # Load the directory and all files in them\n",
    "        f = open(file_pth)\n",
    "        self.data_json = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.length = len(self.data_json)       \n",
    "        self.base_path = os.path.join(root, partition)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_dict = self.data_json[idx]\n",
    "        name = str(cur_dict['id'])\n",
    "\n",
    "        audio_feat = np.load(os.path.join(self.base_path, name+'_feat.npy'))\n",
    "        print(os.path.join(self.base_path, name+'_feat.npy'))\n",
    "        # audio_feat = np.load(os.path.join(self.base_path, name+'_hidden_state.npy'))\n",
    "        # print('**************',audio_feat.shape)\n",
    "        \n",
    "        audio_feat = np.squeeze(audio_feat)\n",
    "\n",
    "        text = [cur_dict['description']]\n",
    "        text_feat = model_text.encode(text)\n",
    "        text_feat = np.squeeze(text_feat)\n",
    "\n",
    "        target = np.load(os.path.join(self.base_path, name+'_target.npy'))\n",
    "        target = np.squeeze(target)\n",
    "        \n",
    "        audio_feat = torch.FloatTensor(audio_feat)\n",
    "        text_feat = torch.FloatTensor(text_feat)\n",
    "        target = torch.LongTensor(target)\n",
    "\n",
    "        sample = {\n",
    "                  \"audio_feat\": audio_feat,\n",
    "                  \"text_feat\": text_feat,\n",
    "                  \"target\": target\n",
    "                }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "\n",
    "        batch_audio = [i[\"audio_feat\"] for i in batch]\n",
    "        batch_text = [i[\"text_feat\"] for i in batch]\n",
    "        batch_target = [i[\"target\"] for i in batch]\n",
    "\n",
    "        batch_audio_pad = pad_sequence(batch_audio, batch_first=True)\n",
    "        lengths_audio = [i.shape[0] for i in batch_audio]\n",
    "\n",
    "        batch_target_pad = pad_sequence(batch_target, batch_first=True)\n",
    "        lengths_target = [i.shape[0] for i in batch_target]\n",
    "\n",
    "        batch_audio_pad = torch.FloatTensor(batch_audio_pad)\n",
    "        batch_text = torch.stack(batch_text)\n",
    "        batch_target_pad = torch.LongTensor(batch_target_pad)\n",
    "\n",
    "        return batch_audio_pad, batch_text, batch_target_pad, torch.tensor(lengths_audio), torch.tensor(lengths_target)\n",
    "\n",
    "\n",
    "root = './'\n",
    "\n",
    "test_data = RASDataset(root, 'test_combined.json', partition= \"test\")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = 1,\n",
    "    collate_fn = test_data.collate_fn,\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.linear(self.fc(x))\n",
    "        # print(outputs.size())\n",
    "        alpha = torch.softmax(outputs, dim=2)\n",
    "        x = (x * alpha)\n",
    "        return x\n",
    "\n",
    "class RASModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "        self.audio_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.text_linear = nn.Linear(384, self.embed_dim)\n",
    "        self.mha_a_t = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                               dropout=self.dropout, batch_first=True)\n",
    "        self.mha_t_a = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                               dropout=self.dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embed_dim*2, 2)\n",
    "        self.fc2 = nn.Linear(self.embed_dim,124)\n",
    "        self.fc3 = nn.Linear(124,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.rnn = nn.LSTM(self.embed_dim,self.embed_dim,2,batch_first=True)\n",
    "        self.concat_linear = nn.Linear(in_features=2 * self.embed_dim, out_features= self.embed_dim)\n",
    "        self.classifier = nn.Linear(in_features= self.embed_dim, out_features=2)\n",
    "        \n",
    "    \n",
    "    def forward(self, audio_fea, text_fea):\n",
    "\n",
    "        B, T, D = audio_fea.size()\n",
    "        text_fea = text_fea[:,None,:]\n",
    "        text_fea_rep = text_fea.repeat(1, T, 1) #B,1, 384 -> B, T, 384\n",
    "\n",
    "        audio_fea = self.audio_linear(audio_fea)\n",
    "        text_fea = self.text_linear(text_fea_rep)\n",
    "\n",
    "        x_a2t, _ = self.mha_a_t(text_fea, audio_fea, audio_fea)\n",
    "        # x_a2t = torch.mean(x_a2t, dim=2)\n",
    "\n",
    "        x_t2a, _ = self.mha_t_a(audio_fea, text_fea, text_fea)\n",
    "        # x_t2a = torch.mean(x_t2a, dim=2)\n",
    "\n",
    "        x = torch.stack((x_a2t, x_t2a), dim=2)\n",
    "        x_mean, x_std = torch.std_mean(x, dim=2)\n",
    "        x = torch.cat((x_mean, x_std), dim=2)  \n",
    "        x = self.concat_linear(x)\n",
    "        # x,_ = self.rnn(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = RASModel(embed_dim = 512, num_heads = 8, dropout=0.2).to(device)\n",
    "model.load_state_dict(torch.load('best.pkl'))\n",
    "\n",
    "class weighted_log_loss(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(weighted_log_loss,self).__init__()\n",
    "        self.LOSS_BIAS = 0.2\n",
    "\n",
    "    def forward(self, yt, yp):   \n",
    "        pos_loss = -(0 + yt) * torch.log(0 + yp + 1e-7)\n",
    "        neg_loss = -(1 - yt) * torch.log(1 - yp + 1e-7)\n",
    "\n",
    "        return self.LOSS_BIAS * torch.mean(neg_loss) + (1. - self.LOSS_BIAS) * torch.mean(pos_loss)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "\n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "\n",
    "    for i, (audio_fea, text_fea, target, audio_len, target_len) in enumerate(dataloader):\n",
    "        if i>1:\n",
    "            return np.argmax(logits, axis=2).squeeze()\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        np.save('input_audio.npy', audio_fea)\n",
    "        audio_fea = audio_fea.to(device)\n",
    "        text_fea  = text_fea.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode(): \n",
    "            ### Forward Propagation\n",
    "            logits  = model(audio_fea, text_fea).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "output   = eval(model, test_loader)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 747, 512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('input_audio.npy')\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('asr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c4b94d2c2ebce001fd4ad053dd500eb853c4ed6e1be56e669d125c68b3ff64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
