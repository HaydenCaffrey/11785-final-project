{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# import librosa\n",
    "# data, sr = sf.read('1.wav')\n",
    "# data = librosa.resample(data[:,0],sr,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\":24,\n",
    "    \"beam_width\" : 2,\n",
    "    \"lr\" : 1e-5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"epochs\" : 100\n",
    "    } # Feel free to add more items here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/home/haozhez2/anaconda3/envs/asr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f99644b9870>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummaryX import summary\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "torch.manual_seed(0)\n",
    "# model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "# model_audio = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "# i= feature_extractor(data, return_tensors=\"pt\", sampling_rate=16000)\n",
    "# #previous are in dataloader\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   o= model_audio(i.input_values)\n",
    "# print(o.keys())\n",
    "# print(o.last_hidden_state.shape)\n",
    "# print(o.extract_features.shape)\n",
    "# print(i.input_values.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_text = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences we want to encode. Example:\n",
    "# sentence = ['This framework generates embeddings for each input sentence.']\n",
    "\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "# text_fea = model_text.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class RASDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, file_pth, partition = 'train', subset= None): \n",
    "        # Load the directory and all files in them\n",
    "        f = open(file_pth)\n",
    "        self.data_json = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.length = len(self.data_json)       \n",
    "        self.base_path = os.path.join(root, partition)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_dict = self.data_json[idx]\n",
    "        name = str(cur_dict['id'])\n",
    "\n",
    "        audio_feat = np.load(os.path.join(self.base_path, name+'_feat.npy'))\n",
    "        audio_feat = np.squeeze(audio_feat)\n",
    "\n",
    "        text = [cur_dict['description']]\n",
    "        text_feat = model_text.encode(text)\n",
    "        text_feat = np.squeeze(text_feat)\n",
    "\n",
    "        target = np.load(os.path.join(self.base_path, name+'_target.npy'))\n",
    "        target = np.squeeze(target)\n",
    "        \n",
    "        audio_feat = torch.FloatTensor(audio_feat)\n",
    "        text_feat = torch.FloatTensor(text_feat)\n",
    "        target = torch.LongTensor(target)\n",
    "\n",
    "        sample = {\n",
    "                  \"audio_feat\": audio_feat,\n",
    "                  \"text_feat\": text_feat,\n",
    "                  \"target\": target\n",
    "                }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "\n",
    "        batch_audio = [i[\"audio_feat\"] for i in batch]\n",
    "        batch_text = [i[\"text_feat\"] for i in batch]\n",
    "        batch_target = [i[\"target\"] for i in batch]\n",
    "\n",
    "        batch_audio_pad = pad_sequence(batch_audio, batch_first=True)\n",
    "        lengths_audio = [i.shape[0] for i in batch_audio]\n",
    "\n",
    "        batch_target_pad = pad_sequence(batch_target, batch_first=True)\n",
    "        lengths_target = [i.shape[0] for i in batch_target]\n",
    "\n",
    "        batch_audio_pad = torch.FloatTensor(batch_audio_pad)\n",
    "        batch_text = torch.stack(batch_text)\n",
    "        batch_target_pad = torch.LongTensor(batch_target_pad)\n",
    "\n",
    "        return batch_audio_pad, batch_text, batch_target_pad, torch.tensor(lengths_audio), torch.tensor(lengths_target)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  24\n",
      "Train dataset samples = 7500, batches = 313\n",
      "Val dataset samples = 1500, batches = 63\n"
     ]
    }
   ],
   "source": [
    "root = './'\n",
    "train_data = RASDataset(root, 'train_combined.json', partition= \"train\")\n",
    "val_data = RASDataset(root, 'val_combined.json', partition= \"val\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'], \n",
    "    collate_fn = train_data.collate_fn,\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'],\n",
    "    collate_fn = val_data.collate_fn,\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1497, 512]) torch.Size([24, 384]) torch.Size([24, 1497]) torch.Size([24]) torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    audio, text, target, laudio, ltarget = data\n",
    "    print(audio.shape, text.shape, target.shape, laudio.shape, ltarget.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.linear(self.fc(x))\n",
    "        # print(outputs.size())\n",
    "        alpha = torch.softmax(outputs, dim=2)\n",
    "        x = (x * alpha)\n",
    "        return x\n",
    "\n",
    "class RASModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.audio_linear = nn.Linear(512, self.embed_dim)\n",
    "        self.text_linear = nn.Linear(384, 384)\n",
    "        # self.attention = nn.MultiheadAttention(self.embed_dim+384, self.num_heads, batch_first=True)\n",
    "        self.attention = Attention(self.embed_dim+384,self.embed_dim+384) #nn.MultiheadAttention(self.embed_dim+384, self.num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embed_dim+384, 2)\n",
    "        self.fc2 = nn.Linear(512,124)\n",
    "        self.fc3 = nn.Linear(124,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.rnn = nn.GRU(self.embed_dim+384,self.embed_dim+384,2,batch_first=True)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, audio_fea, text_fea):\n",
    "\n",
    "        # audio_fea = self.audio_linear(audio_fea)\n",
    "        # text_fea = self.text_linear(text_fea)\n",
    "\n",
    "        B, T, D = audio_fea.size()\n",
    "\n",
    "        text_fea_rep = text_fea.repeat(T, 1) #B,512 -> T,B,512\n",
    "        text_fea_rep = text_fea_rep.reshape(B,T,-1)\n",
    "\n",
    "        x = torch.cat((audio_fea, text_fea_rep), 2)\n",
    "\n",
    "        # x, _ = self.attention(x, x, x)\n",
    "        # x, _ = self.rnn(x)\n",
    "        # print(rnn_output.size())\n",
    "        x = self.attention(x)\n",
    "        # print(att_output.size())\n",
    "        linear_attn = self.fc1(x)\n",
    "        # print(linear_attn.size())\n",
    "        return linear_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "                          Kernel Shape     Output Shape    Params Mult-Adds\n",
      "Layer                                                                      \n",
      "0_attention.fc.Linear_0     [896, 896]  [24, 1497, 896]  803.712k  802.816k\n",
      "1_attention.fc.Tanh_1                -  [24, 1497, 896]         -         -\n",
      "2_attention.Linear_linear     [896, 1]    [24, 1497, 1]     896.0     896.0\n",
      "3_fc1                         [896, 2]    [24, 1497, 2]    1.794k    1.792k\n",
      "----------------------------------------------------------------------------\n",
      "                        Totals\n",
      "Total params          806.402k\n",
      "Trainable params      806.402k\n",
      "Non-trainable params       0.0\n",
      "Mult-Adds             805.504k\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/home/haozhez2/anaconda3/envs/asr/lib/python3.9/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_sum = df.sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_attention.fc.Linear_0</th>\n",
       "      <td>[896, 896]</td>\n",
       "      <td>[24, 1497, 896]</td>\n",
       "      <td>803712.0</td>\n",
       "      <td>802816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_attention.fc.Tanh_1</th>\n",
       "      <td>-</td>\n",
       "      <td>[24, 1497, 896]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_attention.Linear_linear</th>\n",
       "      <td>[896, 1]</td>\n",
       "      <td>[24, 1497, 1]</td>\n",
       "      <td>896.0</td>\n",
       "      <td>896.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_fc1</th>\n",
       "      <td>[896, 2]</td>\n",
       "      <td>[24, 1497, 2]</td>\n",
       "      <td>1794.0</td>\n",
       "      <td>1792.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Kernel Shape     Output Shape    Params  Mult-Adds\n",
       "Layer                                                                       \n",
       "0_attention.fc.Linear_0     [896, 896]  [24, 1497, 896]  803712.0   802816.0\n",
       "1_attention.fc.Tanh_1                -  [24, 1497, 896]       NaN        NaN\n",
       "2_attention.Linear_linear     [896, 1]    [24, 1497, 1]     896.0      896.0\n",
       "3_fc1                         [896, 2]    [24, 1497, 2]    1794.0     1792.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RASModel(512, 8).to(device)\n",
    "summary(model, audio.to(device), text.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_log_loss(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(weighted_log_loss,self).__init__()\n",
    "        self.LOSS_BIAS = 0.2\n",
    "\n",
    "    def forward(self, yt, yp):   \n",
    "        pos_loss = -(0 + yt) * torch.log(0 + yp + 1e-7)\n",
    "        neg_loss = -(1 - yt) * torch.log(1 - yp + 1e-7)\n",
    "\n",
    "        return self.LOSS_BIAS * torch.mean(neg_loss) + (1. - self.LOSS_BIAS) * torch.mean(pos_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.CrossEntropyLoss((torch.FloatTensor([0.45, 1]).to(device)))\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']) # What goes in here?\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
    "    \n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "\n",
    "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    \n",
    "    for i, (audio_fea, text_fea, target, audio_len, target_len) in enumerate(dataloader):\n",
    "        \n",
    "        ### Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        audio_fea = audio_fea.to(device)\n",
    "        text_fea  = text_fea.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ### Forward Propagation\n",
    "        logits  = model(audio_fea, text_fea).permute(0,2,1)\n",
    "\n",
    "        ### Loss Calculation\n",
    "        # for j in range(logits.shape[2]):\n",
    "        #     if j==0:\n",
    "        #         loss  = criterion(logits[:,:,j], target[:,j])\n",
    "        #     else:\n",
    "        #         loss  += criterion(logits[:,:,j], target[:,j])\n",
    "        loss = criterion(logits,target)\n",
    "        ### Backward Propagation\n",
    "        loss.backward() \n",
    "        \n",
    "        ### Gradient Descent\n",
    "        optimizer.step()       \n",
    "\n",
    "        tloss   += loss.item()\n",
    "        for j in range(logits.shape[0]):\n",
    "            # print(torch.argmax(logits[j,:,:audio_len[j]], dim= 0).shape)\n",
    "            # print(target[j,:target_len[j]].shape)\n",
    "            tacc += torch.mean(torch.argmax(logits[j,:,:audio_len[j]], dim= 0) == target[j,:target_len[j]], dtype=torch.float32).item()\n",
    "            prob_all.extend(np.argmax(logits.detach().cpu().numpy()[j,:,:audio_len[j]], axis= 0)) #求每一行的最大值索引\n",
    "            label_all.extend(target.detach().cpu().numpy()[j,:target_len[j]])\n",
    "            # print(f1_score(label_all,prob_all))\n",
    "        # if i%100==0:\n",
    "        #     print(np.sum(torch.argmax(logits, dim= 1).cpu().numpy()))\n",
    "        #     print(np.sum(target.cpu().numpy()))\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))), \n",
    "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1) / logits.shape[0])),\n",
    "                              f1=\"{:.04f}%\".format(float(f1_score(label_all,prob_all)*100)))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del audio_fea, text_fea, target\n",
    "        torch.cuda.empty_cache()\n",
    "  \n",
    "    batch_bar.close()\n",
    "    tloss   /= len(train_loader)\n",
    "    tacc    /= len(train_loader)*config['batch_size']\n",
    "    tf1  = f1_score(label_all,prob_all)\n",
    "    return tloss, tacc, tf1\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "\n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (audio_fea, text_fea, target, audio_len, target_len) in enumerate(dataloader):\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        audio_fea = audio_fea.to(device)\n",
    "        text_fea  = text_fea.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode(): \n",
    "            ### Forward Propagation\n",
    "            logits  = model(audio_fea, text_fea).permute(0,2,1)\n",
    "            ### Loss Calculation\n",
    "            # for j in range(logits.shape[2]):\n",
    "            #     if j==0:\n",
    "            #         loss  = criterion(logits[:,:,j], target[:,j])\n",
    "            #     else:\n",
    "            #         loss  += criterion(logits[:,:,j], target[:,j])\n",
    "            loss    = criterion(logits, target)\n",
    "            # loss = criterion(torch.argmax(logits, dim=1),target)\n",
    "\n",
    "\n",
    "        vloss   += loss.item()\n",
    "        for j in range(logits.shape[0]):\n",
    "            vacc += torch.mean(torch.argmax(logits[j,:,:audio_len[j]], dim= 0) == target[j,:target_len[j]], dtype=torch.float32).item()\n",
    "            prob_all.extend(np.argmax(logits.detach().cpu().numpy()[j,:,:audio_len[j]], axis= 0)) #求每一行的最大值索引\n",
    "            label_all.extend(target.detach().cpu().numpy()[j,:target_len[j]])\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))), \n",
    "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1) / logits.shape[0])),\n",
    "                              f1=\"{:.04f}%\".format(float(f1_score(label_all,prob_all)*100)))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        ### Release memory\n",
    "        del audio_fea, text_fea, target\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss   /= len(val_loader)\n",
    "    vacc    /= len(val_loader)*config['batch_size']\n",
    "    vf1   = f1_score(label_all,prob_all)\n",
    "\n",
    "    return vloss, vacc, vf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 49.8304%\tTrain Loss 0.7334\tTrain F1 40.1473\t Learning Rate 0.0000100\n",
      "\tVal Acc 48.0579%\tVal Loss 0.7194\tVal F1 41.3413\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 48.6303%\tTrain Loss 0.7079\tTrain F1 41.2786\t Learning Rate 0.0000100\n",
      "\tVal Acc 47.8246%\tVal Loss 0.6998\tVal F1 41.6647\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 48.4697%\tTrain Loss 0.6907\tTrain F1 41.4267\t Learning Rate 0.0000100\n",
      "\tVal Acc 47.3160%\tVal Loss 0.6858\tVal F1 42.1752\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  97%|█████████▋| 304/313 [13:54<00:47,  5.29s/it, acc=48.3128%, f1=41.5768%, loss=0.6789]"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc, train_f1   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1   = eval(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\tTrain F1 {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100,\\\n",
    "                                                                                                       train_loss, train_f1*100, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\\tVal F1 {:.04f}\".format(val_acc*100, val_loss, val_f1*100))\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pkl')\n",
    "\n",
    "    if val_acc>best_acc:\n",
    "        torch.save(model.state_dict(), 'best.pkl')\n",
    "        best_acc = val_acc\n",
    "    ### Log metrics at each epoch in your run \n",
    "    # Optionally, you can log at each batch inside train/eval functions \n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    # wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss, \n",
    "            #    'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})    \n",
    "    # wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss, 'lr': curr_lr})\n",
    "\n",
    "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c4b94d2c2ebce001fd4ad053dd500eb853c4ed6e1be56e669d125c68b3ff64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
