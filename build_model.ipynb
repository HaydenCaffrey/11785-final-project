{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# import librosa\n",
    "# data, sr = sf.read('1.wav')\n",
    "# data = librosa.resample(data[:,0],sr,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\":16,\n",
    "    \"beam_width\" : 2,\n",
    "    \"lr\" : 1e-5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"epochs\" : 100\n",
    "    } # Feel free to add more items here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/home/haozhez2/anaconda3/envs/asr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f79b4527830>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummaryX import summary\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "torch.manual_seed(0)\n",
    "# model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "# model_audio = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "# i= feature_extractor(data, return_tensors=\"pt\", sampling_rate=16000)\n",
    "# #previous are in dataloader\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   o= model_audio(i.input_values)\n",
    "# print(o.keys())\n",
    "# print(o.last_hidden_state.shape)\n",
    "# print(o.extract_features.shape)\n",
    "# print(i.input_values.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_text = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences we want to encode. Example:\n",
    "# sentence = ['This framework generates embeddings for each input sentence.']\n",
    "\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "# text_fea = model_text.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class RASDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, file_pth, partition = 'train', subset= None): \n",
    "        # Load the directory and all files in them\n",
    "        f = open(file_pth)\n",
    "        self.data_json = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.length = len(self.data_json)       \n",
    "        self.base_path = os.path.join(root, partition)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_dict = self.data_json[idx]\n",
    "        name = str(cur_dict['id'])\n",
    "\n",
    "        audio_feat = np.load(os.path.join(self.base_path, name+'_feat.npy'))\n",
    "        audio_feat = np.squeeze(audio_feat)\n",
    "\n",
    "        text = [cur_dict['description']]\n",
    "        text_feat = model_text.encode(text)\n",
    "        text_feat = np.squeeze(text_feat)\n",
    "\n",
    "        target = np.load(os.path.join(self.base_path, name+'_target.npy'))\n",
    "        target = np.squeeze(target)\n",
    "        \n",
    "        audio_feat = torch.FloatTensor(audio_feat)\n",
    "        text_feat = torch.FloatTensor(text_feat)\n",
    "        target = torch.LongTensor(target)\n",
    "\n",
    "        sample = {\n",
    "                  \"audio_feat\": audio_feat,\n",
    "                  \"text_feat\": text_feat,\n",
    "                  \"target\": target\n",
    "                }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "\n",
    "        batch_audio = [i[\"audio_feat\"] for i in batch]\n",
    "        batch_text = [i[\"text_feat\"] for i in batch]\n",
    "        batch_target = [i[\"target\"] for i in batch]\n",
    "\n",
    "        batch_audio_pad = pad_sequence(batch_audio, batch_first=True)\n",
    "        lengths_audio = [i.shape[0] for i in batch_audio]\n",
    "\n",
    "        batch_target_pad = pad_sequence(batch_target, batch_first=True)\n",
    "        lengths_target = [i.shape[0] for i in batch_target]\n",
    "\n",
    "        batch_audio_pad = torch.FloatTensor(batch_audio_pad)\n",
    "        batch_text = torch.stack(batch_text)\n",
    "        batch_target_pad = torch.LongTensor(batch_target_pad)\n",
    "\n",
    "        return batch_audio_pad, batch_text, batch_target_pad, torch.tensor(lengths_audio), torch.tensor(lengths_target)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  16\n",
      "Train dataset samples = 7500, batches = 469\n",
      "Val dataset samples = 1500, batches = 94\n"
     ]
    }
   ],
   "source": [
    "root = './'\n",
    "train_data = RASDataset(root, 'train_combined.json', partition= \"train\")\n",
    "val_data = RASDataset(root, 'val_combined.json', partition= \"val\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'], \n",
    "    collate_fn = train_data.collate_fn,\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data, \n",
    "    num_workers = 0,\n",
    "    batch_size  = config['batch_size'],\n",
    "    collate_fn = val_data.collate_fn,\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1497, 512]) torch.Size([16, 384]) torch.Size([16, 1497]) torch.Size([16]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    audio, text, target, laudio, ltarget = data\n",
    "    print(audio.shape, text.shape, target.shape, laudio.shape, ltarget.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, input_size: int, hidden_size: int):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "#         self.linear = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         outputs = self.linear(self.fc(x))\n",
    "#         # print(outputs.size())\n",
    "#         alpha = torch.softmax(outputs, dim=2)\n",
    "#         x = (x * alpha)\n",
    "#         return x\n",
    "\n",
    "# class RASModel(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.num_heads = num_heads\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.audio_linear = nn.Linear(512, self.embed_dim)\n",
    "#         self.text_linear = nn.Linear(384, 384)\n",
    "#         # self.attention = nn.MultiheadAttention(self.embed_dim+384, self.num_heads, batch_first=True)\n",
    "#         self.attention = Attention(self.embed_dim+384,self.embed_dim+384) #nn.MultiheadAttention(self.embed_dim+384, self.num_heads, batch_first=True)\n",
    "#         self.fc1 = nn.Linear(self.embed_dim+384, 2)\n",
    "#         self.fc2 = nn.Linear(512,124)\n",
    "#         self.fc3 = nn.Linear(124,2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.rnn = nn.GRU(self.embed_dim+384,self.embed_dim+384,2,batch_first=True)\n",
    "\n",
    "        \n",
    "    \n",
    "#     def forward(self, audio_fea, text_fea):\n",
    "\n",
    "#         # audio_fea = self.audio_linear(audio_fea)\n",
    "#         # text_fea = self.text_linear(text_fea)\n",
    "\n",
    "#         B, T, D = audio_fea.size()\n",
    "\n",
    "#         text_fea_rep = text_fea.repeat(T, 1) #B,512 -> T,B,512\n",
    "#         text_fea_rep = text_fea_rep.reshape(B,T,-1)\n",
    "\n",
    "#         x = torch.cat((audio_fea, text_fea_rep), 2)\n",
    "\n",
    "#         # x, _ = self.attention(x, x, x)\n",
    "#         # x, _ = self.rnn(x)\n",
    "#         # print(rnn_output.size())\n",
    "#         x = self.attention(x)\n",
    "#         # print(att_output.size())\n",
    "#         linear_attn = self.fc1(x)\n",
    "#         # print(linear_attn.size())\n",
    "#         return linear_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.linear(self.fc(x))\n",
    "        # print(outputs.size())\n",
    "        alpha = torch.softmax(outputs, dim=2)\n",
    "        x = (x * alpha)\n",
    "        return x\n",
    "\n",
    "class RASModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "        self.audio_linear = nn.Linear(512, self.embed_dim)\n",
    "        self.text_linear = nn.Linear(384, self.embed_dim)\n",
    "        self.mha_a_t = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                               dropout=self.dropout, batch_first=True)\n",
    "        self.mha_t_a = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                               dropout=self.dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embed_dim*2, 2)\n",
    "        self.fc2 = nn.Linear(512,124)\n",
    "        self.fc3 = nn.Linear(124,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.rnn = nn.GRU(self.embed_dim+384,self.embed_dim+384,2,batch_first=True)\n",
    "        self.concat_linear = nn.Linear(in_features=2 * self.embed_dim, out_features= self.embed_dim)\n",
    "        self.classifier = nn.Linear(in_features= self.embed_dim, out_features=2)\n",
    "        \n",
    "    \n",
    "    def forward(self, audio_fea, text_fea):\n",
    "\n",
    "        B, T, D = audio_fea.size()\n",
    "        text_fea = text_fea[:,None,:]\n",
    "        text_fea_rep = text_fea.repeat(1, T, 1) #B,1, 384 -> B, T, 384\n",
    "\n",
    "        audio_fea = self.audio_linear(audio_fea)\n",
    "        text_fea = self.text_linear(text_fea_rep)\n",
    "\n",
    "        x_a2t, _ = self.mha_a_t(text_fea, audio_fea, audio_fea)\n",
    "        # x_a2t = torch.mean(x_a2t, dim=2)\n",
    "\n",
    "        x_t2a, _ = self.mha_t_a(audio_fea, text_fea, text_fea)\n",
    "        # x_t2a = torch.mean(x_t2a, dim=2)\n",
    "\n",
    "        x = torch.stack((x_a2t, x_t2a), dim=2)\n",
    "        x_mean, x_std = torch.std_mean(x, dim=2)\n",
    "        x = torch.cat((x_mean, x_std), dim=2)  \n",
    "        x = self.concat_linear(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "                Kernel Shape     Output Shape  Params  Mult-Adds\n",
      "Layer                                                           \n",
      "0_audio_linear    [512, 512]  [16, 1497, 512]  262656     262144\n",
      "1_text_linear     [384, 512]  [16, 1497, 512]  197120     196608\n",
      "2_concat_linear  [1024, 512]  [16, 1497, 512]  524800     524288\n",
      "3_classifier        [512, 2]    [16, 1497, 2]    1026       1024\n",
      "----------------------------------------------------------------\n",
      "                      Totals\n",
      "Total params          985602\n",
      "Trainable params      985602\n",
      "Non-trainable params       0\n",
      "Mult-Adds             984064\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/home/haozhez2/anaconda3/envs/asr/lib/python3.9/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_sum = df.sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_audio_linear</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[16, 1497, 512]</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_text_linear</th>\n",
       "      <td>[384, 512]</td>\n",
       "      <td>[16, 1497, 512]</td>\n",
       "      <td>197120</td>\n",
       "      <td>196608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_concat_linear</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[16, 1497, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_classifier</th>\n",
       "      <td>[512, 2]</td>\n",
       "      <td>[16, 1497, 2]</td>\n",
       "      <td>1026</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Kernel Shape     Output Shape  Params  Mult-Adds\n",
       "Layer                                                           \n",
       "0_audio_linear    [512, 512]  [16, 1497, 512]  262656     262144\n",
       "1_text_linear     [384, 512]  [16, 1497, 512]  197120     196608\n",
       "2_concat_linear  [1024, 512]  [16, 1497, 512]  524800     524288\n",
       "3_classifier        [512, 2]    [16, 1497, 2]    1026       1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RASModel(embed_dim = 512, num_heads = 8, dropout=0.2).to(device)\n",
    "summary(model, audio.to(device), text.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_log_loss(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(weighted_log_loss,self).__init__()\n",
    "        self.LOSS_BIAS = 0.2\n",
    "\n",
    "    def forward(self, yt, yp):   \n",
    "        pos_loss = -(0 + yt) * torch.log(0 + yp + 1e-7)\n",
    "        neg_loss = -(1 - yt) * torch.log(1 - yp + 1e-7)\n",
    "\n",
    "        return self.LOSS_BIAS * torch.mean(neg_loss) + (1. - self.LOSS_BIAS) * torch.mean(pos_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.CrossEntropyLoss((torch.FloatTensor([0.3, 1]).to(device)))\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']) # What goes in here?\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
    "    \n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "\n",
    "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    \n",
    "    for i, (audio_fea, text_fea, target, audio_len, target_len) in enumerate(dataloader):\n",
    "        \n",
    "        ### Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        audio_fea = audio_fea.to(device)\n",
    "        text_fea  = text_fea.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ### Forward Propagation\n",
    "        logits  = model(audio_fea, text_fea).permute(0,2,1)\n",
    "\n",
    "        ### Loss Calculation\n",
    "        # for j in range(logits.shape[2]):\n",
    "        #     if j==0:\n",
    "        #         loss  = criterion(logits[:,:,j], target[:,j])\n",
    "        #     else:\n",
    "        #         loss  += criterion(logits[:,:,j], target[:,j])\n",
    "        loss = criterion(logits,target)\n",
    "        ### Backward Propagation\n",
    "        loss.backward() \n",
    "        \n",
    "        ### Gradient Descent\n",
    "        optimizer.step()       \n",
    "\n",
    "        tloss   += loss.item()\n",
    "        for j in range(logits.shape[0]):\n",
    "            # print(torch.argmax(logits[j,:,:audio_len[j]], dim= 0).shape)\n",
    "            # print(target[j,:target_len[j]].shape)\n",
    "            tacc += torch.mean(torch.argmax(logits[j,:,:audio_len[j]], dim= 0) == target[j,:target_len[j]], dtype=torch.float32).item()\n",
    "            prob_all.extend(np.argmax(logits.detach().cpu().numpy()[j,:,:audio_len[j]], axis= 0)) #求每一行的最大值索引\n",
    "            label_all.extend(target.detach().cpu().numpy()[j,:target_len[j]])\n",
    "            # print(f1_score(label_all,prob_all))\n",
    "        # if i%100==0:\n",
    "        #     print(np.sum(torch.argmax(logits, dim= 1).cpu().numpy()))\n",
    "        #     print(np.sum(target.cpu().numpy()))\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))), \n",
    "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1) / logits.shape[0])),\n",
    "                              f1=\"{:.04f}%\".format(float(f1_score(label_all,prob_all)*100)))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del audio_fea, text_fea, target\n",
    "        torch.cuda.empty_cache()\n",
    "  \n",
    "    batch_bar.close()\n",
    "    tloss   /= len(train_loader)\n",
    "    tacc    /= len(train_loader)*config['batch_size']\n",
    "    tf1  = f1_score(label_all,prob_all)\n",
    "    return tloss, tacc, tf1\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "\n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (audio_fea, text_fea, target, audio_len, target_len) in enumerate(dataloader):\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        audio_fea = audio_fea.to(device)\n",
    "        text_fea  = text_fea.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode(): \n",
    "            ### Forward Propagation\n",
    "            logits  = model(audio_fea, text_fea).permute(0,2,1)\n",
    "            ### Loss Calculation\n",
    "            # for j in range(logits.shape[2]):\n",
    "            #     if j==0:\n",
    "            #         loss  = criterion(logits[:,:,j], target[:,j])\n",
    "            #     else:\n",
    "            #         loss  += criterion(logits[:,:,j], target[:,j])\n",
    "            loss    = criterion(logits, target)\n",
    "            # loss = criterion(torch.argmax(logits, dim=1),target)\n",
    "\n",
    "\n",
    "        vloss   += loss.item()\n",
    "        for j in range(logits.shape[0]):\n",
    "            vacc += torch.mean(torch.argmax(logits[j,:,:audio_len[j]], dim= 0) == target[j,:target_len[j]], dtype=torch.float32).item()\n",
    "            prob_all.extend(np.argmax(logits.detach().cpu().numpy()[j,:,:audio_len[j]], axis= 0)) #求每一行的最大值索引\n",
    "            label_all.extend(target.detach().cpu().numpy()[j,:target_len[j]])\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))), \n",
    "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1) / logits.shape[0])),\n",
    "                              f1=\"{:.04f}%\".format(float(f1_score(label_all,prob_all)*100)))\n",
    "\n",
    "        batch_bar.update()\n",
    "    \n",
    "        ### Release memory\n",
    "        del audio_fea, text_fea, target\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss   /= len(val_loader)\n",
    "    vacc    /= len(val_loader)*config['batch_size']\n",
    "    vf1   = f1_score(label_all,prob_all)\n",
    "\n",
    "    return vloss, vacc, vf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 40.6894%\tTrain Loss 0.6823\tTrain F1 49.0273\t Learning Rate 0.0000100\n",
      "\tVal Acc 48.3911%\tVal Loss 0.6779\tVal F1 48.1537\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 48.4362%\tTrain Loss 0.6778\tTrain F1 48.1073\t Learning Rate 0.0000100\n",
      "\tVal Acc 50.0187%\tVal Loss 0.6763\tVal F1 48.0085\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 22/469 [00:17<06:41,  1.11it/s, acc=48.7405%, f1=47.8587%, loss=0.6765]"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc, train_f1   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1   = eval(model, val_loader)\n",
    "    test_loss, test_acc, test_f1   = eval(model, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\tTrain F1 {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100,\\\n",
    "                                                                                                       train_loss, train_f1*100, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\\tVal F1 {:.04f}\".format(val_acc*100, val_loss, val_f1*100))\n",
    "    print(\"\\tTest Acc {:.04f}%\\tTest Loss {:.04f}\\tTest F1 {:.04f}\".format(test_acc*100, \\\n",
    "        test_loss, test_f1*100))\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pkl')\n",
    "\n",
    "    if val_acc>best_acc:\n",
    "        torch.save(model.state_dict(), 'best.pkl')\n",
    "        best_acc = val_acc\n",
    "    ### Log metrics at each epoch in your run \n",
    "    # Optionally, you can log at each batch inside train/eval functions \n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    # wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss, \n",
    "            #    'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})    \n",
    "    # wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss, 'lr': curr_lr})\n",
    "\n",
    "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c4b94d2c2ebce001fd4ad053dd500eb853c4ed6e1be56e669d125c68b3ff64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
